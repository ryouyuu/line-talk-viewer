import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from janome.tokenizer import Tokenizer
from janome.analyzer import Analyzer
from janome.charfilter import UnicodeNormalizeCharFilter
from janome.tokenfilter import POSKeepFilter, LowerCaseFilter
import re

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmotionAnalyzer:
    """æ„Ÿæƒ…åˆ†æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """æ„Ÿæƒ…åˆ†æå™¨ã‚’åˆæœŸåŒ–"""
        self.model_loaded = False
        self.sentiment_analyzer = None
        logger.info("æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã¯å¿…è¦æ™‚ã«èª­ã¿è¾¼ã¿ã¾ã™")
    
    def analyze_text(self, text: str) -> Dict[str, float]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æ„Ÿæƒ…åˆ†æçµæœï¼ˆpositive, negative, neutralã®ç¢ºç‡ï¼‰
        """
        if not self.model_loaded or not text.strip():
            return {'positive': 0.33, 'negative': 0.33, 'neutral': 0.34}
        
        try:
            # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ†å‰²
            if len(text) > 500:
                text = text[:500]
            
            result = self.sentiment_analyzer(text)
            
            # çµæœã‚’æ­£è¦åŒ–
            if result[0]['label'] == 'positive':
                return {'positive': 0.8, 'negative': 0.1, 'neutral': 0.1}
            elif result[0]['label'] == 'negative':
                return {'positive': 0.1, 'negative': 0.8, 'neutral': 0.1}
            else:
                return {'positive': 0.1, 'negative': 0.1, 'neutral': 0.8}
                
        except Exception as e:
            logger.error(f"æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {'positive': 0.33, 'negative': 0.33, 'neutral': 0.34}
    
    def analyze_messages(self, df: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrameã®æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œï¼ˆãƒãƒƒãƒå‡¦ç†å¯¾å¿œï¼‰
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            
        Returns:
            æ„Ÿæƒ…åˆ†æçµæœã‚’å«ã‚€DataFrame
        """
        if df.empty:
            return df
        
        # æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’å¿…è¦æ™‚ã«èª­ã¿è¾¼ã¿
        if not self.model_loaded:
            try:
                from transformers import pipeline
                self.sentiment_analyzer = pipeline(
                    "sentiment-analysis",
                    model="nlptown/bert-base-multilingual-uncased-sentiment",
                    device=-1  # CPUä½¿ç”¨
                )
                self.model_loaded = True
                logger.info("æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            except Exception as e:
                logger.warning(f"æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                self.model_loaded = False
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é™¤å¤–
        message_df = df[df['type'] != 'system'].copy()
        system_df = df[df['type'] == 'system'].copy()
        
        if message_df.empty:
            return df
        
        results = []
        
        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, len(message_df), batch_size):
            batch = message_df.iloc[i:i+batch_size]
            messages = batch['message'].tolist()
            
            try:
                # ãƒãƒƒãƒã§æ„Ÿæƒ…åˆ†æå®Ÿè¡Œ
                if self.model_loaded:
                    batch_results = self.sentiment_analyzer(messages)
                    
                    for result in batch_results:
                        if result['label'] == 'positive':
                            results.append({'positive': 0.8, 'negative': 0.1, 'neutral': 0.1})
                        elif result['label'] == 'negative':
                            results.append({'positive': 0.1, 'negative': 0.8, 'neutral': 0.1})
                        else:
                            results.append({'positive': 0.1, 'negative': 0.1, 'neutral': 0.8})
                else:
                    # ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    for _ in range(len(messages)):
                        results.append({'positive': 0.33, 'negative': 0.33, 'neutral': 0.34})
                        
            except Exception as e:
                logger.error(f"ãƒãƒƒãƒæ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                for _ in range(len(messages)):
                    results.append({'positive': 0.33, 'negative': 0.33, 'neutral': 0.34})
            
            # é€²æ—è¡¨ç¤º
            logger.info(f"æ„Ÿæƒ…åˆ†æé€²æ—: {min(i + batch_size, len(message_df))}/{len(message_df)}")
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã®çµæœã‚’è¿½åŠ 
        for _ in range(len(system_df)):
            results.append({'positive': 0, 'negative': 0, 'neutral': 1})
        
        # çµæœã‚’DataFrameã«è¿½åŠ 
        emotion_df = pd.DataFrame(results)
        df_with_emotion = pd.concat([df, emotion_df], axis=1)
        
        return df_with_emotion
    
    def get_daily_emotion_summary(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ—¥åˆ¥æ„Ÿæƒ…åˆ†æã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        
        Args:
            df: æ„Ÿæƒ…åˆ†ææ¸ˆã¿DataFrame
            
        Returns:
            æ—¥åˆ¥æ„Ÿæƒ…é›†è¨ˆDataFrame
        """
        if df.empty or 'positive' not in df.columns:
            return pd.DataFrame()
        
        daily_emotion = df.groupby('date').agg({
            'positive': 'sum',
            'negative': 'sum', 
            'neutral': 'sum'
        }).reset_index()
        
        return daily_emotion

class WordAnalyzer:
    """é »å‡ºãƒ¯ãƒ¼ãƒ‰åˆ†æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """ãƒ¯ãƒ¼ãƒ‰åˆ†æå™¨ã‚’åˆæœŸåŒ–"""
        self.tokenizer = Tokenizer()
        
        # åˆ†æå¯¾è±¡å¤–ã®å“è©
        self.exclude_pos = ['åŠ©è©', 'åŠ©å‹•è©', 'è¨˜å·', 'æ¥ç¶šåŠ©è©', 'å‰¯åŠ©è©']
        
        # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ã™ãã‚‹å˜èªï¼‰
        self.stop_words = {
            'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã§', 'ã¨', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š',
            'ã‚„', 'ã‹', 'ã‚‚', 'ãªã©', 'ã£ã¦', 'ã§ã™', 'ã¾ã™', 'ã ', 'ã§ã™',
            'ãŠ', 'ã”', 'ã•ã‚“', 'ã¡ã‚ƒã‚“', 'ãã‚“', 'ã­', 'ã‚ˆ', 'ãª', 'ã‚',
            'ã‚', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‚“', 'ã£', 'ãƒ¼', 'ï¼', 'ï¼Ÿ', 'ã€‚',
            'ã€', 'ã€Œ', 'ã€', 'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€', 'ã€', 'ãƒ»'
        }
    
    def extract_words(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ã‚’æŠ½å‡º
        
        Args:
            text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸåè©ãƒªã‚¹ãƒˆ
        """
        if not text.strip():
            return []
        
        try:
            # å½¢æ…‹ç´ è§£æ
            tokens = self.tokenizer.tokenize(text)
            
            words = []
            for token in tokens:
                # å“è©æƒ…å ±ã‚’å–å¾—
                pos = token.part_of_speech.split(',')[0]
                
                # åè©ã®ã¿ã‚’æŠ½å‡ºï¼ˆé™¤å¤–å“è©ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                if pos == 'åè©' and token.surface not in self.stop_words:
                    # 1æ–‡å­—ã®å˜èªã¯é™¤å¤–
                    if len(token.surface) > 1:
                        words.append(token.surface)
            
            return words
            
        except Exception as e:
            logger.error(f"ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def analyze_messages(self, df: pd.DataFrame) -> Dict[str, int]:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrameã‹ã‚‰é »å‡ºãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            
        Returns:
            ãƒ¯ãƒ¼ãƒ‰é »åº¦è¾æ›¸
        """
        if df.empty:
            return {}
        
        word_freq = {}
        
        for idx, row in df.iterrows():
            if row['type'] == 'system':
                continue
            
            words = self.extract_words(row['message'])
            
            for word in words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # é€²æ—è¡¨ç¤º
            if (idx + 1) % 50 == 0:
                logger.info(f"ãƒ¯ãƒ¼ãƒ‰åˆ†æé€²æ—: {idx + 1}/{len(df)}")
        
        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        
        return dict(sorted_words)
    
    def get_speaker_word_freq(self, df: pd.DataFrame, speaker: str) -> Dict[str, int]:
        """
        ç‰¹å®šç™ºè¨€è€…ã®é »å‡ºãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            speaker: ç™ºè¨€è€…å
            
        Returns:
            ãƒ¯ãƒ¼ãƒ‰é »åº¦è¾æ›¸
        """
        speaker_df = df[df['sender'] == speaker].copy()
        return self.analyze_messages(speaker_df)
    
    def get_daily_word_freq(self, df: pd.DataFrame, target_date: str) -> Dict[str, int]:
        """
        ç‰¹å®šæ—¥ä»˜ã®é »å‡ºãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            target_date: å¯¾è±¡æ—¥ä»˜
            
        Returns:
            ãƒ¯ãƒ¼ãƒ‰é »åº¦è¾æ›¸
        """
        daily_df = df[df['date'] == target_date].copy()
        return self.analyze_messages(daily_df)

class TopicAnalyzer:
    """ãƒˆãƒ”ãƒƒã‚¯åˆ†æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹ï¼ˆGPT APIä½¿ç”¨ï¼‰"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        ãƒˆãƒ”ãƒƒã‚¯åˆ†æå™¨ã‚’åˆæœŸåŒ–
        
        Args:
            api_key: OpenAI APIã‚­ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        self.api_key = api_key
        self.client = None
        
        if api_key:
            try:
                import openai
                self.client = openai.OpenAI(api_key=api_key)
                logger.info("OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
            except Exception as e:
                logger.warning(f"OpenAI APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def summarize_daily_conversation(self, df: pd.DataFrame, target_date: str) -> str:
        """
        ç‰¹å®šæ—¥ä»˜ã®ä¼šè©±ã‚’è¦ç´„
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            target_date: å¯¾è±¡æ—¥ä»˜
            
        Returns:
            è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self.client:
            return "GPT APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        daily_df = df[df['date'] == target_date].copy()
        
        if daily_df.empty:
            return "ãã®æ—¥ã®ä¼šè©±ãŒã‚ã‚Šã¾ã›ã‚“"
        
        # ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        conversation = []
        for _, row in daily_df.iterrows():
            if row['type'] == 'message':
                conversation.append(f"{row['sender']}: {row['message']}")
        
        if not conversation:
            return "æœ‰åŠ¹ãªä¼šè©±ãŒã‚ã‚Šã¾ã›ã‚“"
        
        conversation_text = "\n".join(conversation)
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": "ã‚ãªãŸã¯ä¼šè©±åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸä¼šè©±ã‚’1-2æ–‡ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
                    },
                    {
                        "role": "user", 
                        "content": f"ä»¥ä¸‹ã®ä¼šè©±ã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n\n{conversation_text}"
                    }
                ],
                max_tokens=100,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"GPT APIè¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            return "è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸ"

class ConversationAnalyzer:
    """ä¼šè©±åˆ†æãƒ»çµ±è¨ˆã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """ä¼šè©±åˆ†æå™¨ã‚’åˆæœŸåŒ–"""
        # çµµæ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.emoji_pattern = re.compile(
            r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000027BF\U0001F900-\U0001F9FF\U0001F018-\U0001F270]'
        )
        
        # ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.stamp_pattern = re.compile(r'\[ã‚¹ã‚¿ãƒ³ãƒ—\]')
        
        # æ™‚é–“å¸¯ã®å®šç¾©
        self.time_periods = {
            'æ—©æœ': (5, 8),
            'åˆå‰': (8, 12),
            'æ˜¼': (12, 14),
            'åˆå¾Œ': (14, 18),
            'å¤•æ–¹': (18, 20),
            'å¤œ': (20, 23),
            'æ·±å¤œ': (23, 5)
        }
    
    def analyze_time_distribution(self, df: pd.DataFrame) -> Dict[str, int]:
        """
        æ™‚é–“å¸¯åˆ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ†å¸ƒã‚’åˆ†æ
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            
        Returns:
            æ™‚é–“å¸¯åˆ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
        """
        if df.empty:
            return {}
        
        # æ™‚é–“ã‚’æŠ½å‡º
        df['hour'] = pd.to_datetime(df['time'], format='%H:%M').dt.hour
        
        time_dist = {}
        
        for period, (start, end) in self.time_periods.items():
            if start < end:
                # é€šå¸¸ã®æ™‚é–“å¸¯ï¼ˆä¾‹ï¼š8-12æ™‚ï¼‰
                count = len(df[(df['hour'] >= start) & (df['hour'] < end)])
            else:
                # æ·±å¤œï¼ˆ23-5æ™‚ï¼‰
                count = len(df[(df['hour'] >= start) | (df['hour'] < end)])
            
            time_dist[period] = count
        
        return time_dist
    
    def analyze_message_length(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·åˆ†æ
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            
        Returns:
            ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·çµ±è¨ˆ
        """
        if df.empty:
            return {}
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã‚’è¨ˆç®—
        df['length'] = df['message'].str.len()
        
        # å…¨ä½“çµ±è¨ˆ
        overall_stats = {
            'å¹³å‡æ–‡å­—æ•°': df['length'].mean(),
            'æœ€å¤§æ–‡å­—æ•°': df['length'].max(),
            'æœ€å°æ–‡å­—æ•°': df['length'].min(),
            'ä¸­å¤®å€¤': df['length'].median()
        }
        
        # é•·ã•åˆ¥åˆ†é¡
        length_categories = {
            'çŸ­ã„ï¼ˆ1-10æ–‡å­—ï¼‰': len(df[df['length'] <= 10]),
            'ä¸­ç¨‹åº¦ï¼ˆ11-50æ–‡å­—ï¼‰': len(df[(df['length'] > 10) & (df['length'] <= 50)]),
            'é•·ã„ï¼ˆ51-100æ–‡å­—ï¼‰': len(df[(df['length'] > 50) & (df['length'] <= 100)]),
            'ã¨ã¦ã‚‚é•·ã„ï¼ˆ100æ–‡å­—ä»¥ä¸Šï¼‰': len(df[df['length'] > 100])
        }
        
        # ç™ºè¨€è€…åˆ¥çµ±è¨ˆ
        speaker_stats = {}
        for speaker in df['sender'].unique():
            speaker_df = df[df['sender'] == speaker]
            speaker_stats[speaker] = {
                'å¹³å‡æ–‡å­—æ•°': speaker_df['length'].mean(),
                'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°': len(speaker_df),
                'ç·æ–‡å­—æ•°': speaker_df['length'].sum()
            }
        
        return {
            'å…¨ä½“çµ±è¨ˆ': overall_stats,
            'é•·ã•åˆ¥åˆ†é¡': length_categories,
            'ç™ºè¨€è€…åˆ¥çµ±è¨ˆ': speaker_stats
        }
    
    def analyze_emoji_usage(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """
        çµµæ–‡å­—ãƒ»ã‚¹ã‚¿ãƒ³ãƒ—ä½¿ç”¨åˆ†æ
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            
        Returns:
            çµµæ–‡å­—ãƒ»ã‚¹ã‚¿ãƒ³ãƒ—ä½¿ç”¨çµ±è¨ˆ
        """
        if df.empty:
            return {}
        
        emoji_stats = {
            'çµµæ–‡å­—ä½¿ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°': 0,
            'ã‚¹ã‚¿ãƒ³ãƒ—ä½¿ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°': 0,
            'çµµæ–‡å­—ç·æ•°': 0,
            'ã‚¹ã‚¿ãƒ³ãƒ—ç·æ•°': 0,
            'ã‚ˆãä½¿ã†çµµæ–‡å­—': {},
            'ç™ºè¨€è€…åˆ¥çµµæ–‡å­—ä½¿ç”¨': {}
        }
        
        for idx, row in df.iterrows():
            if row['type'] == 'system':
                continue
            
            message = row['message']
            sender = row['sender']
            
            # çµµæ–‡å­—æ¤œå‡º
            emojis = self.emoji_pattern.findall(message)
            if emojis:
                emoji_stats['çµµæ–‡å­—ä½¿ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°'] += 1
                emoji_stats['çµµæ–‡å­—ç·æ•°'] += len(emojis)
                
                # ç™ºè¨€è€…åˆ¥çµ±è¨ˆ
                if sender not in emoji_stats['ç™ºè¨€è€…åˆ¥çµµæ–‡å­—ä½¿ç”¨']:
                    emoji_stats['ç™ºè¨€è€…åˆ¥çµµæ–‡å­—ä½¿ç”¨'][sender] = 0
                emoji_stats['ç™ºè¨€è€…åˆ¥çµµæ–‡å­—ä½¿ç”¨'][sender] += len(emojis)
                
                # å€‹åˆ¥çµµæ–‡å­—çµ±è¨ˆ
                for emoji in emojis:
                    emoji_stats['ã‚ˆãä½¿ã†çµµæ–‡å­—'][emoji] = emoji_stats['ã‚ˆãä½¿ã†çµµæ–‡å­—'].get(emoji, 0) + 1
            
            # ã‚¹ã‚¿ãƒ³ãƒ—æ¤œå‡º
            if self.stamp_pattern.search(message):
                emoji_stats['ã‚¹ã‚¿ãƒ³ãƒ—ä½¿ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°'] += 1
                emoji_stats['ã‚¹ã‚¿ãƒ³ãƒ—ç·æ•°'] += 1
        
        # ã‚ˆãä½¿ã†çµµæ–‡å­—ã‚’ä¸Šä½10å€‹ã«åˆ¶é™
        emoji_stats['ã‚ˆãä½¿ã†çµµæ–‡å­—'] = dict(
            sorted(emoji_stats['ã‚ˆãä½¿ã†çµµæ–‡å­—'].items(), 
                   key=lambda x: x[1], reverse=True)[:10]
        )
        
        return emoji_stats
    
    def analyze_response_time(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """
        è¿”ä¿¡é€Ÿåº¦åˆ†æ
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            
        Returns:
            è¿”ä¿¡é€Ÿåº¦çµ±è¨ˆ
        """
        if df.empty:
            return {}
        
        # æ—¥æ™‚ã‚’çµåˆã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])
        df = df.sort_values('datetime')
        
        response_times = []
        speaker_response_times = {}
        
        for i in range(1, len(df)):
            current_msg = df.iloc[i]
            prev_msg = df.iloc[i-1]
            
            # åŒã˜æ—¥ã§ã€ç•°ãªã‚‹ç™ºè¨€è€…ã®å ´åˆ
            if (current_msg['date'] == prev_msg['date'] and 
                current_msg['sender'] != prev_msg['sender']):
                
                time_diff = (current_msg['datetime'] - prev_msg['datetime']).total_seconds() / 60  # åˆ†å˜ä½
                
                if time_diff <= 60:  # 1æ™‚é–“ä»¥å†…ã®è¿”ä¿¡ã®ã¿
                    response_times.append(time_diff)
                    
                    # ç™ºè¨€è€…åˆ¥çµ±è¨ˆ
                    responder = current_msg['sender']
                    if responder not in speaker_response_times:
                        speaker_response_times[responder] = []
                    speaker_response_times[responder].append(time_diff)
        
        # çµ±è¨ˆè¨ˆç®—
        if response_times:
            overall_stats = {
                'å¹³å‡è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': np.mean(response_times),
                'ä¸­å¤®å€¤è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': np.median(response_times),
                'æœ€çŸ­è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': np.min(response_times),
                'æœ€é•·è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': np.max(response_times),
                'è¿”ä¿¡å›æ•°': len(response_times)
            }
        else:
            overall_stats = {
                'å¹³å‡è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': 0,
                'ä¸­å¤®å€¤è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': 0,
                'æœ€çŸ­è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': 0,
                'æœ€é•·è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': 0,
                'è¿”ä¿¡å›æ•°': 0
            }
        
        # ç™ºè¨€è€…åˆ¥çµ±è¨ˆ
        speaker_stats = {}
        for speaker, times in speaker_response_times.items():
            speaker_stats[speaker] = {
                'å¹³å‡è¿”ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰': np.mean(times),
                'è¿”ä¿¡å›æ•°': len(times)
            }
        
        return {
            'å…¨ä½“çµ±è¨ˆ': overall_stats,
            'ç™ºè¨€è€…åˆ¥çµ±è¨ˆ': speaker_stats
        }
    
    def analyze_message_speed(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """
        è¿”ä¿¡é€Ÿåº¦åˆ†æï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡é€Ÿåº¦åˆ†æï¼‰
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            
        Returns:
            è¿”ä¿¡é€Ÿåº¦çµ±è¨ˆ
        """
        if df.empty:
            return {}
        
        # æ—¥æ™‚ã‚’çµåˆã—ã¦datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])
        df = df.sort_values('datetime')
        
        # ç™ºè¨€è€…åˆ¥ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡é–“éš”ã‚’åˆ†æ
        speaker_speeds = {}
        conversation_speeds = []
        
        for speaker in df['sender'].unique():
            speaker_df = df[df['sender'] == speaker].copy()
            speaker_df = speaker_df.sort_values('datetime')
            
            if len(speaker_df) < 2:
                continue
            
            # åŒä¸€ç™ºè¨€è€…ã®é€£ç¶šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ã‚’è¨ˆç®—
            intervals = []
            for i in range(1, len(speaker_df)):
                time_diff = (speaker_df.iloc[i]['datetime'] - speaker_df.iloc[i-1]['datetime']).total_seconds() / 60
                if time_diff <= 30:  # 30åˆ†ä»¥å†…ã®é€£ç¶šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿
                    intervals.append(time_diff)
            
            if intervals:
                speaker_speeds[speaker] = {
                    'å¹³å‡é€ä¿¡é–“éš”ï¼ˆåˆ†ï¼‰': np.mean(intervals),
                    'æœ€çŸ­é€ä¿¡é–“éš”ï¼ˆåˆ†ï¼‰': np.min(intervals),
                    'æœ€é•·é€ä¿¡é–“éš”ï¼ˆåˆ†ï¼‰': np.max(intervals),
                    'é€£ç¶šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°': len(intervals) + 1,
                    'é€ä¿¡é€Ÿåº¦ãƒ¬ãƒ™ãƒ«': self._get_speed_level(np.mean(intervals))
                }
        
        # ä¼šè©±å…¨ä½“ã®ãƒ†ãƒ³ãƒåˆ†æ
        all_intervals = []
        for i in range(1, len(df)):
            time_diff = (df.iloc[i]['datetime'] - df.iloc[i-1]['datetime']).total_seconds() / 60
            if time_diff <= 60:  # 1æ™‚é–“ä»¥å†…ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”
                all_intervals.append(time_diff)
        
        if all_intervals:
            overall_speed_stats = {
                'å¹³å‡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ï¼ˆåˆ†ï¼‰': np.mean(all_intervals),
                'ä¸­å¤®å€¤ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ï¼ˆåˆ†ï¼‰': np.median(all_intervals),
                'æœ€çŸ­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ï¼ˆåˆ†ï¼‰': np.min(all_intervals),
                'æœ€é•·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ï¼ˆåˆ†ï¼‰': np.max(all_intervals),
                'ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°': len(df),
                'ä¼šè©±ãƒ†ãƒ³ãƒãƒ¬ãƒ™ãƒ«': self._get_conversation_tempo_level(np.mean(all_intervals))
            }
        else:
            overall_speed_stats = {
                'å¹³å‡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ï¼ˆåˆ†ï¼‰': 0,
                'ä¸­å¤®å€¤ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ï¼ˆåˆ†ï¼‰': 0,
                'æœ€çŸ­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ï¼ˆåˆ†ï¼‰': 0,
                'æœ€é•·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é–“éš”ï¼ˆåˆ†ï¼‰': 0,
                'ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°': len(df),
                'ä¼šè©±ãƒ†ãƒ³ãƒãƒ¬ãƒ™ãƒ«': 'ä¸æ˜'
            }
        
        # æ™‚é–“å¸¯åˆ¥ã®é€ä¿¡é€Ÿåº¦åˆ†æ
        try:
            df['hour'] = pd.to_datetime(df['time'], format='%H:%M').dt.hour
        except Exception as e:
            logger.warning(f"æ™‚é–“å½¢å¼ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ–‡å­—åˆ—ã‹ã‚‰æ™‚é–“ã‚’æŠ½å‡º
            df['hour'] = df['time'].str.extract(r'(\d{1,2}):').astype(int)
        
        hourly_speeds = {}
        
        for hour in range(24):
            hourly_df = df[df['hour'] == hour]
            if len(hourly_df) > 1:
                hourly_intervals = []
                hourly_df = hourly_df.sort_values('datetime')
                for i in range(1, len(hourly_df)):
                    time_diff = (hourly_df.iloc[i]['datetime'] - hourly_df.iloc[i-1]['datetime']).total_seconds() / 60
                    if time_diff <= 30:
                        hourly_intervals.append(time_diff)
                
                if hourly_intervals:
                    hourly_speeds[hour] = {
                        'å¹³å‡é–“éš”ï¼ˆåˆ†ï¼‰': np.mean(hourly_intervals),
                        'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°': len(hourly_df),
                        'é€Ÿåº¦ãƒ¬ãƒ™ãƒ«': self._get_speed_level(np.mean(hourly_intervals))
                    }
        
        return {
            'å…¨ä½“çµ±è¨ˆ': overall_speed_stats,
            'ç™ºè¨€è€…åˆ¥é€Ÿåº¦': speaker_speeds,
            'æ™‚é–“å¸¯åˆ¥é€Ÿåº¦': hourly_speeds
        }
    
    def _get_speed_level(self, avg_interval: float) -> str:
        """é€ä¿¡é€Ÿåº¦ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
        if avg_interval <= 1:
            return "è¶…é«˜é€Ÿ ğŸš€"
        elif avg_interval <= 3:
            return "é«˜é€Ÿ âš¡"
        elif avg_interval <= 10:
            return "ä¸­é€Ÿ ğŸƒ"
        elif avg_interval <= 30:
            return "ä½é€Ÿ ğŸš¶"
        else:
            return "è¶…ä½é€Ÿ ğŸŒ"
    
    def _get_conversation_tempo_level(self, avg_interval: float) -> str:
        """ä¼šè©±ãƒ†ãƒ³ãƒãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
        if avg_interval <= 2:
            return "è¶…æ´»ç™º ğŸ”¥"
        elif avg_interval <= 5:
            return "æ´»ç™º ğŸ’¬"
        elif avg_interval <= 15:
            return "æ™®é€š ğŸ“±"
        elif avg_interval <= 60:
            return "ã‚†ã£ãŸã‚Š ğŸ˜Œ"
        else:
            return "é™ã‹ ğŸ¤«"
    
    def analyze_seasonal_patterns(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """
        å­£ç¯€æ€§åˆ†æï¼ˆæœˆåˆ¥ãƒ»æ›œæ—¥åˆ¥ï¼‰
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            
        Returns:
            å­£ç¯€æ€§çµ±è¨ˆ
        """
        if df.empty:
            return {}
        
        try:
            # æ—¥ä»˜ã‚’datetimeã«å¤‰æ›
            df['datetime'] = pd.to_datetime(df['date'])
            df['month'] = df['datetime'].dt.month
            df['weekday'] = df['datetime'].dt.dayofweek
        except Exception as e:
            logger.error(f"å­£ç¯€æ€§åˆ†æã§ã®æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
        
        # æœˆåˆ¥çµ±è¨ˆ
        monthly_stats = df.groupby('month').agg({
            'message': 'count',
            'sender': 'nunique'
        }).rename(columns={'message': 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°', 'sender': 'å‚åŠ è€…æ•°'})
        
        # æ›œæ—¥åˆ¥çµ±è¨ˆ
        weekday_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
        weekday_stats = df.groupby('weekday').agg({
            'message': 'count',
            'sender': 'nunique'
        }).rename(columns={'message': 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°', 'sender': 'å‚åŠ è€…æ•°'})
        weekday_stats.index = [weekday_names[i] for i in weekday_stats.index]
        
        return {
            'æœˆåˆ¥çµ±è¨ˆ': monthly_stats.to_dict('index'),
            'æ›œæ—¥åˆ¥çµ±è¨ˆ': weekday_stats.to_dict('index')
        }
    
    def get_conversation_summary(self, df: pd.DataFrame) -> Dict[str, any]:
        """
        ä¼šè©±å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            
        Returns:
            ä¼šè©±ã‚µãƒãƒªãƒ¼
        """
        if df.empty:
            return {}
        
        # åŸºæœ¬çµ±è¨ˆ
        basic_stats = {
            'ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°': len(df),
            'ä¼šè©±æ—¥æ•°': df['date'].nunique(),
            'å‚åŠ è€…æ•°': df['sender'].nunique(),
            'æœŸé–“': f"{df['date'].min()} ã€œ {df['date'].max()}"
        }
        
        # å„åˆ†æã‚’å®Ÿè¡Œ
        time_dist = self.analyze_time_distribution(df)
        length_stats = self.analyze_message_length(df)
        emoji_stats = self.analyze_emoji_usage(df)
        response_stats = self.analyze_response_time(df)
        speed_stats = self.analyze_message_speed(df)
        seasonal_stats = self.analyze_seasonal_patterns(df)
        
        return {
            'åŸºæœ¬çµ±è¨ˆ': basic_stats,
            'æ™‚é–“å¸¯åˆ†å¸ƒ': time_dist,
            'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·çµ±è¨ˆ': length_stats,
            'çµµæ–‡å­—ãƒ»ã‚¹ã‚¿ãƒ³ãƒ—çµ±è¨ˆ': emoji_stats,
            'è¿”ä¿¡é€Ÿåº¦çµ±è¨ˆ': response_stats,
            'é€ä¿¡é€Ÿåº¦çµ±è¨ˆ': speed_stats,
            'å­£ç¯€æ€§çµ±è¨ˆ': seasonal_stats
        }

class SearchFilter:
    """æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ã‚’åˆæœŸåŒ–"""
        pass
    
    def filter_by_date_range(self, df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame:
        """
        æ—¥ä»˜ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            start_date: é–‹å§‹æ—¥ï¼ˆYYYY/MM/DDï¼‰
            end_date: çµ‚äº†æ—¥ï¼ˆYYYY/MM/DDï¼‰
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty:
            return df
        
        try:
            start_dt = pd.to_datetime(start_date)
            end_dt = pd.to_datetime(end_date)
            
            df['date_dt'] = pd.to_datetime(df['date'])
            filtered_df = df[(df['date_dt'] >= start_dt) & (df['date_dt'] <= end_dt)].copy()
            filtered_df = filtered_df.drop('date_dt', axis=1)
            
            return filtered_df
        except Exception as e:
            logger.error(f"æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
            return df
    
    def filter_by_speaker(self, df: pd.DataFrame, speakers: List[str]) -> pd.DataFrame:
        """
        é€ä¿¡è€…ã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            speakers: é€ä¿¡è€…ãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty or not speakers:
            return df
        
        return df[df['sender'].isin(speakers)].copy()
    
    def filter_by_message_type(self, df: pd.DataFrame, message_types: List[str]) -> pd.DataFrame:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            message_types: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ãƒªã‚¹ãƒˆï¼ˆ'text', 'stamp', 'image', 'system'ï¼‰
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty or not message_types:
            return df
        
        filtered_df = df.copy()
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        def get_message_type(row):
            message = row['message']
            if row['type'] == 'system':
                return 'system'
            elif '[ã‚¹ã‚¿ãƒ³ãƒ—]' in message:
                return 'stamp'
            elif any(keyword in message.lower() for keyword in ['ç”»åƒ', 'image', 'å†™çœŸ']):
                return 'image'
            else:
                return 'text'
        
        filtered_df['message_type'] = filtered_df.apply(get_message_type, axis=1)
        filtered_df = filtered_df[filtered_df['message_type'].isin(message_types)]
        filtered_df = filtered_df.drop('message_type', axis=1)
        
        return filtered_df
    
    def filter_by_emotion(self, df: pd.DataFrame, emotion_type: str, threshold: float = 0.5) -> pd.DataFrame:
        """
        æ„Ÿæƒ…ã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            emotion_type: æ„Ÿæƒ…ã‚¿ã‚¤ãƒ—ï¼ˆ'positive', 'negative', 'neutral'ï¼‰
            threshold: é–¾å€¤
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty or emotion_type not in ['positive', 'negative', 'neutral']:
            return df
        
        if emotion_type not in df.columns:
            return df
        
        return df[df[emotion_type] >= threshold].copy()
    
    def filter_by_length(self, df: pd.DataFrame, min_length: int = 0, max_length: int = None) -> pd.DataFrame:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            min_length: æœ€å°æ–‡å­—æ•°
            max_length: æœ€å¤§æ–‡å­—æ•°ï¼ˆNoneã®å ´åˆã¯åˆ¶é™ãªã—ï¼‰
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty:
            return df
        
        df['length'] = df['message'].str.len()
        
        if max_length is None:
            filtered_df = df[df['length'] >= min_length].copy()
        else:
            filtered_df = df[(df['length'] >= min_length) & (df['length'] <= max_length)].copy()
        
        filtered_df = filtered_df.drop('length', axis=1)
        return filtered_df
    
    def filter_by_keyword(self, df: pd.DataFrame, keywords: List[str], case_sensitive: bool = False) -> pd.DataFrame:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            keywords: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
            case_sensitive: å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã™ã‚‹ã‹
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty or not keywords:
            return df
        
        def contains_keyword(message):
            if case_sensitive:
                return any(keyword in message for keyword in keywords)
            else:
                return any(keyword.lower() in message.lower() for keyword in keywords)
        
        return df[df['message'].apply(contains_keyword)].copy()
    
    def filter_by_time_range(self, df: pd.DataFrame, start_time: str, end_time: str) -> pd.DataFrame:
        """
        æ™‚é–“ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            start_time: é–‹å§‹æ™‚é–“ï¼ˆHH:MMï¼‰
            end_time: çµ‚äº†æ™‚é–“ï¼ˆHH:MMï¼‰
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty:
            return df
        
        try:
            df['time_dt'] = pd.to_datetime(df['time'], format='%H:%M')
            start_dt = pd.to_datetime(start_time, format='%H:%M')
            end_dt = pd.to_datetime(end_time, format='%H:%M')
            
            if start_dt <= end_dt:
                # é€šå¸¸ã®æ™‚é–“ç¯„å›²ï¼ˆä¾‹ï¼š09:00-18:00ï¼‰
                filtered_df = df[(df['time_dt'] >= start_dt) & (df['time_dt'] <= end_dt)].copy()
            else:
                # æ·±å¤œã‚’ã¾ãŸãæ™‚é–“ç¯„å›²ï¼ˆä¾‹ï¼š23:00-05:00ï¼‰
                filtered_df = df[(df['time_dt'] >= start_dt) | (df['time_dt'] <= end_dt)].copy()
            
            filtered_df = filtered_df.drop('time_dt', axis=1)
            return filtered_df
        except Exception as e:
            logger.error(f"æ™‚é–“ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
            return df
    
    def filter_by_emoji(self, df: pd.DataFrame, has_emoji: bool = True) -> pd.DataFrame:
        """
        çµµæ–‡å­—ã®æœ‰ç„¡ã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            has_emoji: çµµæ–‡å­—ã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹ã‹
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty:
            return df
        
        emoji_pattern = re.compile(
            r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U00027BF\U0001F900-\U0001F9FF\U0001F018-\U0001F270]'
        )
        
        def contains_emoji(message):
            return bool(emoji_pattern.search(message))
        
        if has_emoji:
            return df[df['message'].apply(contains_emoji)].copy()
        else:
            return df[~df['message'].apply(contains_emoji)].copy()
    
    def apply_multiple_filters(self, df: pd.DataFrame, filters: Dict) -> pd.DataFrame:
        """
        è¤‡æ•°ã®ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
        
        Args:
            df: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸DataFrame
            filters: ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šè¾æ›¸
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿DataFrame
        """
        if df.empty:
            return df
        
        filtered_df = df.copy()
        
        # æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿
        if 'date_range' in filters:
            start_date, end_date = filters['date_range']
            filtered_df = self.filter_by_date_range(filtered_df, start_date, end_date)
        
        # é€ä¿¡è€…ãƒ•ã‚£ãƒ«ã‚¿
        if 'speakers' in filters:
            filtered_df = self.filter_by_speaker(filtered_df, filters['speakers'])
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
        if 'message_types' in filters:
            filtered_df = self.filter_by_message_type(filtered_df, filters['message_types'])
        
        # æ„Ÿæƒ…ãƒ•ã‚£ãƒ«ã‚¿
        if 'emotion' in filters:
            emotion_type, threshold = filters['emotion']
            filtered_df = self.filter_by_emotion(filtered_df, emotion_type, threshold)
        
        # é•·ã•ãƒ•ã‚£ãƒ«ã‚¿
        if 'length' in filters:
            min_len, max_len = filters['length']
            filtered_df = self.filter_by_length(filtered_df, min_len, max_len)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
        if 'keywords' in filters:
            keywords, case_sensitive = filters['keywords']
            filtered_df = self.filter_by_keyword(filtered_df, keywords, case_sensitive)
        
        # æ™‚é–“ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿
        if 'time_range' in filters:
            start_time, end_time = filters['time_range']
            filtered_df = self.filter_by_time_range(filtered_df, start_time, end_time)
        
        # çµµæ–‡å­—ãƒ•ã‚£ãƒ«ã‚¿
        if 'has_emoji' in filters:
            filtered_df = self.filter_by_emoji(filtered_df, filters['has_emoji'])
        
        return filtered_df

def create_sample_emotion_data() -> pd.DataFrame:
    """ãƒ†ã‚¹ãƒˆç”¨ã®æ„Ÿæƒ…åˆ†æã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    sample_data = {
        'date': ['2025/01/15', '2025/01/15', '2025/01/16', '2025/01/16'],
        'positive': [3, 2, 1, 2],
        'negative': [0, 1, 0, 1], 
        'neutral': [1, 0, 2, 1]
    }
    return pd.DataFrame(sample_data)

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("æ„Ÿæƒ…åˆ†æå™¨ãƒ†ã‚¹ãƒˆ...")
    emotion_analyzer = EmotionAnalyzer()
    
    test_texts = [
        "ãŠã¯ã‚ˆã†ï¼ä»Šæ—¥ã¯æ™´ã‚Œã¦ã‚‹ã­",
        "ã†ã‚“ï¼æ•£æ­©ã«è¡Œã“ã†ã‚ˆ", 
        "æ¥½ã—ã‹ã£ãŸï¼ã¾ãŸæ˜æ—¥ã‚‚é ‘å¼µã‚ã†ã­"
    ]
    
    for text in test_texts:
        result = emotion_analyzer.analyze_text(text)
        print(f"'{text}' -> {result}")
    
    print("\nãƒ¯ãƒ¼ãƒ‰åˆ†æå™¨ãƒ†ã‚¹ãƒˆ...")
    word_analyzer = WordAnalyzer()
    
    test_text = "ãŠã¯ã‚ˆã†ï¼ä»Šæ—¥ã¯å…¬åœ’ã§æ•£æ­©ã«è¡Œã“ã†ã‚ˆ"
    words = word_analyzer.extract_words(test_text)
    print(f"'{test_text}' -> {words}") 